<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <title>Wei Jin</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="Wei Jin">
<meta property="og:url" content="http://withwsf.github.io/index.html">
<meta property="og:site_name" content="Wei Jin">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Wei Jin">
<meta name="twitter:description">
  
    <link rel="alternative" href="/atom.xml" title="Wei Jin" type="application/atom+xml">
  
  
    <link rel="icon" href="http://www.picgifs.com/clip-art/cartoons/south-park/clip-art-south-park-362717.jpg">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>

<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="http://img01.deviantart.net/8e9f/i/2012/194/0/c/jake_the_dog_desktop_picture_by_partack-d571ypf.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Withwsf</a></h1>
		</hgroup>

		
		<p class="header-subtitle">We don&#39;t konw our destiny</p>
		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
						<div class="icon-wrap icon-me hide" data-idx="3">
							<div class="user"></div>
							<div class="shoulder"></div>
						</div>
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>標籤</li>
						
						
						<li>關於</li>
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/wei_jin" title="zhihu">zhihu</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/C/" style="font-size: 16.67px;">C++</a> <a href="/tags/Caffe/" style="font-size: 16.67px;">Caffe</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/Torch/" style="font-size: 13.33px;">Torch</a> <a href="/tags/数据挖掘/" style="font-size: 10px;">数据挖掘</a> <a href="/tags/机器学习/" style="font-size: 20px;">机器学习</a> <a href="/tags/深度学习/" style="font-size: 13.33px;">深度学习</a> <a href="/tags/阅读经典/" style="font-size: 10px;">阅读经典</a>
					</div>
				</section>
				
				
				

				
				
				<section class="switch-part switch-part3">
				
					<div id="js-aboutme">基本上无害 (withwsf#gmail.com)</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">Withwsf</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
			
				<img lazy-src="http://img01.deviantart.net/8e9f/i/2012/194/0/c/jake_the_dog_desktop_picture_by_partack-d571ypf.jpg" class="js-avatar">
			
			</div>
			<hgroup>
			  <h1 class="header-author">Withwsf</h1>
			</hgroup>
			
			<p class="header-subtitle">We don&#39;t konw our destiny</p>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/wei_jin" title="zhihu">zhihu</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
  
    <article id="post-大数据仁波切的live笔记" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/11/29/大数据仁波切的live笔记/" class="article-date">
  	<time datetime="2016-11-29T14:41:13.000Z" itemprop="datePublished">2016-11-29</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/29/大数据仁波切的live笔记/">大数据仁波切的live笔记</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>毫无疑问，<a href="https://www.zhihu.com/people/bei-ming-cheng-hai-sheng" target="_blank" rel="external">大数据仁波切</a>刘鹏的知乎live《如何成为数据科学家？》是一场好live。支撑上述结论的主要有两点：</p>
<ul>
<li><strong>价格便宜</strong>，开始前购买只要两块三毛四。</li>
<li><strong>不装逼</strong>。</li>
</ul>
<p>而青年人也有两个显著特点，一是没钱，二是容易被忽悠，所以，这是一场好live，除了广告有些多。我也不打算记一篇完整的笔记，只挑一些自己觉得有意思的点分享一下。</p>
<h3 id="u5173_u4E8E_u5927_u6570_u636E_u7684_u6982_u5FF5"><a href="#u5173_u4E8E_u5927_u6570_u636E_u7684_u6982_u5FF5" class="headerlink" title="关于大数据的概念"></a><strong>关于大数据的概念</strong></h3><p>大数据这个概念既不是工业界也不是学术界提出的，而是咨询公司<a href="http://tjx.ec.zjczxy.cn/files/2013/01/%E5%A4%A7%E6%95%B0%E6%8D%AE-%E4%B8%8B%E4%B8%80%E4%B8%AA%E5%88%9B%E6%96%B0%E3%80%81%E7%AB%9E%E4%BA%89%E5%92%8C%E7%94%9F%E4%BA%A7%E5%8A%9B%E7%9A%84%E5%89%8D%E6%B2%BF.pdf" target="_blank" rel="external">提出来并炒热的</a>。虽然谷歌是大数据方面的一哥与PR高手，但并没有像炒作AI一样炒作大数据这个概念，美帝的学术界对此更是不感冒。</p>
<p>关于大数据的定义也是众说纷纭，有人认为数据量大就是大数据，还是有人认为大数据有<a href="http://zhishi.moojnn.com/article/26" target="_blank" rel="external">4V</a>的特点，实际上无论是数据量大还是4V标准，都是非常模糊的标准，不能用来衡量一个具体是否是大数据问题。</p>
<p>仁波切给出了数据科学家视角下大数据概念的定义：</p>
<p><img src="https://pic3.zhimg.com/v2-5db55921d202d8c0a9b372033915ff7e_r.jpg" alt="大数据的本质"></p>
<ul>
<li><p><strong>使用行为数据</strong>。行为数据是区别于交易数据的一个概念。交易数据即<br>业务过程中必须记录的数据，例如电信运营商纪录的用户的通话纪录、充值记录、扣费日志等。行为数据并不是必须纪录的数据，比如用户的地理位置、浏览记录等。</p>
</li>
<li><p><strong>全量加工</strong>。大数据问题一般无法通过传统的少量抽样的方式来解决，需要使用全量数据。</p>
</li>
<li><strong>自动化应用</strong>。自动化应用是相对于洞察应用而言的。所谓洞察应用即将数据可视化成人可理解的报表等形式，为后续的运营决策提供参考。自动化应用是数据-&gt;机器的过程，数据自动决策。计算广告、个人征信都是典型的自动化应用。</li>
</ul>
<h3 id="u6570_u636E_u79D1_u5B66_u5BB6_u7684_u5B9A_u4E49_u4E0E_u6838_u5FC3_u7ADE_u4E89_u529B"><a href="#u6570_u636E_u79D1_u5B66_u5BB6_u7684_u5B9A_u4E49_u4E0E_u6838_u5FC3_u7ADE_u4E89_u529B" class="headerlink" title="数据科学家的定义与核心竞争力"></a><strong>数据科学家的定义与核心竞争力</strong></h3><p>工业界传统的数据挖掘是非常强调领域知识的，甚至推崇规则甚于算法。而仁波切定义的数据科学家区别于传统的数据挖掘工程师——</p>
<p>“数据科学家是指采用<strong>科学</strong>的方法论，调动充足的<strong>计算</strong>能力，将<strong>大量</strong>人类无法直接处理的数据转化为有用信息，以驱动<strong>自动化</strong>业务决策的专家”。</p>
<p>另外，需要补充一点，“科学家”在这里只是从事数据或算法相关工作的工程师的别称，请千万不要误会。定义中强调了“科学的方法论”，有意忽略了经验，数据科学家虽然不是真正的科学家，但也要有摆脱各种tricks的泥潭的追求。</p>
<p>一个数据科学家，当然要懂统计、最优化、分布式计算、常见机器学习算法的原理与应用，还要有领域知识，但除了领域知识以外其他技能点都是没有门槛的，智力正常的人看看书、做做练习都能够掌握，在相关领域进行实践之后，领域知识获取也没有太大难度。</p>
<p>真正能够区分普通数据工作者与优秀数据工作者的核心竞争力是<strong>建模能力</strong>,更通俗地说是<strong>定义损失函数</strong>的能力。选择已有的机器学习算法应用到新问题可以做优秀，但无法做到最顶尖，“高玩都是自定义配置的”。</p>
<h3 id="u6570_u636E_u79D1_u5B66_u5BB6_u7684_u517B_u6210_u9014_u5F84"><a href="#u6570_u636E_u79D1_u5B66_u5BB6_u7684_u517B_u6210_u9014_u5F84" class="headerlink" title="数据科学家的养成途径"></a><strong>数据科学家的养成途径</strong></h3><p><img src="https://pic2.zhimg.com/v2-13ea14ce06246293e4b5a7ced416743d_b.jpg" alt=""></p>
<p>仁波切用三层金字塔表示了数据科学家的养成途径，从下往上分别是技能、能力和意识。</p>
<ul>
<li><strong>技能</strong>。基础中的基础，自学或跟课程都可以完成。</li>
<li><strong>能力</strong>。需要在实践中培养，抓住一个问题，无论是广告还是推荐或者征信，做熟做透，有意培养建模能力，一样熟了，培养了感性认识，其他的也不难。</li>
<li><strong>意识</strong>。技能需要刻意练习，意识当然也要特意培养。数据优于经验、计算优于人工无非就是强调数据科学家要相信数据、相信机器，安身立命的根基怎么能不相信呢？</li>
</ul>
<h3 id="u81EA_u5B66_u7684_u8D44_u6599_u63A8_u8350"><a href="#u81EA_u5B66_u7684_u8D44_u6599_u63A8_u8350" class="headerlink" title="自学的资料推荐"></a><strong>自学的资料推荐</strong></h3><p><img src="https://pic2.zhimg.com/v2-7fbcc502d8d35db573b13d7014cdfea5_b.jpg" alt=""></p>
<p>仁波切强调了学习资料不在多，而是将优秀的资料啃透。仁波切博士毕业，毕业后也泡过学术界，推荐的资料门槛较高，可能不是适合所有人。但是，建议是很对的，读透一本书胜过读一百本书的序言。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Caffe中的Net类是如何工作的" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/05/24/Caffe中的Net类是如何工作的/" class="article-date">
  	<time datetime="2016-05-24T12:21:47.000Z" itemprop="datePublished">2016-05-24</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/05/24/Caffe中的Net类是如何工作的/">Caffe中的Net类是如何工作的</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Net类是Caffe中Blobs，Layers，Nets三个抽象层次中最高层的抽象。Nets类负责按照网络定义文件将需要的layers和中间blobs进行实例化，并将所有的Layers组合成一个有向无环图。Nets还提供了在整个网络上进行前向传播与后向传播的接口。下面从观察Net运行的角度来解析一下Net类如何工作。</p>
<h3 id="Net_u7C7B_u6570_u636E_u6210_u5458_u6982_u8FF0"><a href="#Net_u7C7B_u6570_u636E_u6210_u5458_u6982_u8FF0" class="headerlink" title="Net类数据成员概述"></a>Net类数据成员概述</h3><p><strong>下面对Net类中比较重要的数据成员进行说明：</strong></p>
<ul>
<li><p><strong><code>vector&lt;shared_ptr&lt;Layer&lt;Dtype&gt; &gt; &gt; layers_;</code></strong><br>layers_中存放着网络的所有layers，也就是Net类的实例保存着网络定义文件中所有layer的实例</p>
</li>
<li><p><strong><code>vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; blobs_;</code></strong><br>blobs_中保存着网络所有的中间结果，即所有layer的输入数据（bottom blob）和输出数据（top blob）</p>
</li>
<li><p><strong><code>vector&lt;vector&lt;Blob&lt;Dtype&gt;*&gt; &gt; bottom_vecs_;</code></strong></p>
</li>
<li><p><strong><code>vector&lt;vector&lt;Blob&lt;Dtype&gt;*&gt; &gt; top_vecs_;</code></strong><br><code>bottom_vecs_</code>保存的是各个layer的bottom blob的指针，这些指针指向<code>blobs_</code>中的blob。<code>bottom_ves.size()</code>与网络layer的数量相等，由于layer可能有多个bottom blob，所以使用<code>vector&lt;Blob&lt;Dtype&gt;*&gt;</code>来存放layer-wise的bottom blob。同理可以知道<code>top_vecs</code>的作用。</p>
</li>
<li><p><strong><code>vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; params_;</code></strong></p>
</li>
<li><strong><code>vector&lt;Blob&lt;Dtype&gt;*&gt; learnable_params_;</code></strong><br>上述两个数据成员存放的是指向网络参数的指针，注意，直接拥有参数的是layer，<code>params_</code>保存的只是网络中各个layer的参数的指针；而<code>learnable_params_</code>也如其名字所指，保存的是各个layer中可以被学习的参数。</li>
</ul>
<h3 id="Net_u7C7B_u7684_u5B9E_u4F8B_u5316_28_u4E00_u4E2A_u7F51_u7EDC_u7684_u5EFA_u7ACB_29"><a href="#Net_u7C7B_u7684_u5B9E_u4F8B_u5316_28_u4E00_u4E2A_u7F51_u7EDC_u7684_u5EFA_u7ACB_29" class="headerlink" title="Net类的实例化(一个网络的建立)"></a>Net类的实例化(一个网络的建立)</h3><h4 id="u6784_u9020_u51FD_u6570"><a href="#u6784_u9020_u51FD_u6570" class="headerlink" title="构造函数"></a>构造函数</h4><p>Net类有两个构造函数，分别是<code>Net(const NetParameter&amp; param, const Net* root_net)</code>和<code>Net(const string&amp; param_file, Phase phase, const Net* root_net)</code>,前者接受<code>NetParameter</code>的const引用作为参数(后面参数root_net与多GPU并行训练有关，忽略掉并不影响理解)，后者接受定义网络prototxt文件路径和<code>phase</code>作为输入。<br>前者直接调用<code>Init()</code>函数，后者将prototxt文件解析为<code>NetPrameter</code>后调用<code>Init()</code>函数。</p>
<h4 id="Init_28_29_u51FD_u6570"><a href="#Init_28_29_u51FD_u6570" class="headerlink" title="Init()函数"></a>Init()函数</h4><p><code>Init()</code>函数承担初始化一个网络的任务，摘取主干代码描述如下（忽略细节，大致描述过程）：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> layer_id = <span class="number">0</span>; layer_id &lt; param.layer_size(); ++layer_id) &#123;<span class="comment">//param是网络参数，layer_size()返回网络拥有的层数</span></span><br><span class="line">    <span class="keyword">const</span> LayerParameter&amp; layer_param = param.layer(layer_id);<span class="comment">//获取当前layer的参数</span></span><br><span class="line">    layers_.push_back(LayerRegistry&lt;Dtype&gt;::CreateLayer(layer_param));<span class="comment">//根据参数实例化layer</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//下面的两个for循环将此layer的bottom blob的指针和top blob的指针放入bottom_vecs_和top_vecs_,bottom blob和top blob的实例全都存放在blobs_中。相邻的两层，前一层的top blob是后一层的bottom blob，所以blobs_的同一个blob既可能是bottom blob，也可能使top blob。</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> bottom_id = <span class="number">0</span>; bottom_id &lt; layer_param.bottom_size();++bottom_id) &#123;</span><br><span class="line">       <span class="keyword">const</span> <span class="keyword">int</span> blob_id=AppendBottom(param,layer_id,bottom_id,&amp;available_blobs,&amp;blob_name_to_idx);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> top_id = <span class="number">0</span>; top_id &lt; num_top; ++top_id) &#123;</span><br><span class="line">       AppendTop(param, layer_id, top_id, &amp;available_blobs, &amp;blob_name_to_idx);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//接下来的工作是将每层的parameter的指针塞进params_，尤其是learnable_params_。</span></span><br><span class="line">   <span class="keyword">const</span> <span class="keyword">int</span> num_param_blobs = layers_[layer_id]-&gt;blobs().size();</span><br><span class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> param_id = <span class="number">0</span>; param_id &lt; num_param_blobs; ++param_id) &#123;</span><br><span class="line">       AppendParam(param, layer_id, param_id);</span><br><span class="line">       <span class="comment">//AppendParam负责具体的dirtywork</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="u521D_u59CB_u5316_u4E4B_u540E"><a href="#u521D_u59CB_u5316_u4E4B_u540E" class="headerlink" title="初始化之后"></a>初始化之后</h4><p>经过上述过程的网络，参数都是随机产生或者指定的，如果进行预测或这fine-tuning，就需要将载入预训练的权值，Net类提供的函数<code>CopyTrainedLayersFrom(const string&amp; trained_file)</code>可以实现这个过程。</p>
<h3 id="u7F51_u7EDC_u7684_u8FD0_u884C_28_u524D_u5411_u4F20_u64AD_2C__u53CD_u5411_u4F20_u64AD_u548C_u6743_u503C_u66F4_u65B0_29"><a href="#u7F51_u7EDC_u7684_u8FD0_u884C_28_u524D_u5411_u4F20_u64AD_2C__u53CD_u5411_u4F20_u64AD_u548C_u6743_u503C_u66F4_u65B0_29" class="headerlink" title="网络的运行(前向传播, 反向传播和权值更新)"></a>网络的运行(前向传播, 反向传播和权值更新)</h3><p>Net类可以提供网络级的前向前向传播、反向传播和权值更新(即在网络的所有层上有序执行前述动作)。</p>
<h4 id="u524D_u5411_u4F20_u64AD"><a href="#u524D_u5411_u4F20_u64AD" class="headerlink" title="前向传播"></a>前向传播</h4><p>与前向传播相关的函数有<code>Forward(const vector&lt;Blob&lt;Dtype&gt;*&gt; &amp; bottom, Dtype* loss)</code>,<code>Forward(Dtype* loss)</code>,<code>ForwardTo(int end)</code>，<code>ForwardFrom(int start)</code>和<code>ForwardFromTo(int start, int end)</code>，前面的四个函数都是对第五个函数封装，第五个函数定义如下：<br>    <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line">Dtype Net&lt;Dtype&gt;::ForwardFromTo(<span class="keyword">int</span> start, <span class="keyword">int</span> end) &#123;</span><br><span class="line">  CHECK_GE(start, <span class="number">0</span>);</span><br><span class="line">  CHECK_LT(end, layers_.size());</span><br><span class="line">  Dtype loss = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = start; i &lt;= end; ++i) &#123;</span><br><span class="line">    <span class="comment">// LOG(ERROR) &lt;&lt; "Forwarding " &lt;&lt; layer_names_[i];</span></span><br><span class="line">    Dtype layer_loss = layers_[i]-&gt;Forward(bottom_vecs_[i], top_vecs_[i]);</span><br><span class="line">    loss += layer_loss;</span><br><span class="line">    <span class="keyword">if</span> (debug_info_) &#123; ForwardDebugInfo(i); &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> loss;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<pre><code>重点语句是`layers_[i]-&gt;Forward(bottom_vecs_[i], top_vecs_[i]);`，使用layer对应bottom blob和top blob进行前向传播。
</code></pre><h4 id="u53CD_u5411_u4F20_u64AD"><a href="#u53CD_u5411_u4F20_u64AD" class="headerlink" title="反向传播"></a>反向传播</h4><p>与前向传播一样，反向传播也有很多相关函数，但都是对<code>BackwardFromTo(int start, int end)</code>的封装。<br><figure class="highlight"><figcaption><span>Net<dtype>::BackwardFromTo(int start, int end) &#123;</dtype></span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">  CHECK_GE(end, 0);&#10;  CHECK_LT(start, layers_.size());&#10;  for (int i = start; i &#62;= end; --i) &#123;&#10;    if (layer_need_backward_[i]) &#123;&#10;      layers_[i]-&#62;Backward(top_vecs_[i], bottom_need_backward_[i], bottom_vecs_[i]);&#10;      if (debug_info_) &#123; BackwardDebugInfo(i); &#125;&#10;    &#125;&#10;  &#125;&#10;&#125;</span><br></pre></td></tr></table></figure></p>
<p>与前向传播相反，反向传播是从尾到头进行的。</p>
<h4 id="u6743_u503C_u66F4_u65B0"><a href="#u6743_u503C_u66F4_u65B0" class="headerlink" title="权值更新"></a>权值更新</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> Net&lt;Dtype&gt;::Update() &#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; learnable_params_.size(); ++i) &#123;</span><br><span class="line">    learnable_params_[i]-&gt;Update();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在训练的过程中layer的权值要根据反向传播并累积的梯度进行更新，更新的过程由<code>Update()</code>完成。这个函数的功能十分明确，对每个存储learnable_parms的blob调用blob的<code>Update()</code>函数，来更新权值。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/C/">C++</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Caffe/">Caffe</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-在Windows阅读Caffe代码" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/05/23/在Windows阅读Caffe代码/" class="article-date">
  	<time datetime="2016-05-23T07:50:33.000Z" itemprop="datePublished">2016-05-23</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/05/23/在Windows阅读Caffe代码/">在Windows中阅读Caffe代码</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="u524D_u8A00"><a href="#u524D_u8A00" class="headerlink" title="前言"></a>前言</h4><p>我们都知道世界上最好的IDE是Visual Studio，如果能用VS来调试、阅读Caffe的代码，观察Caffe运行过程，相信理解的效率会大大提高，但将Caffe移植到Windows上非常耗时耗力。<br>最近微软官方发布了Caffe的Windows branch，使用Nuget自动下载配置Caffe各种依赖，使得Caffe在Windows下的安装运行比在原生的Linux环境下更加简单。本文勾勒一下从安装到调试运行的大致步骤，希望能对后来者有帮助。</p>
<h4 id="Caffe_u7684_u5B89_u88C5"><a href="#Caffe_u7684_u5B89_u88C5" class="headerlink" title="Caffe的安装"></a>Caffe的安装</h4><ul>
<li>安装Visual Studio 2013</li>
<li><a href="https://github.com/BVLC/caffe/tree/windows" target="_blank" rel="external">下载</a>Windows branch的git文件，并按照README.md中的指导进行安装(如果不使用CUDA，记得按照指导修改“CommonSettings.props”文件)。<h4 id="u8C03_u8BD5_u4E0E_u8FD0_u884C"><a href="#u8C03_u8BD5_u4E0E_u8FD0_u884C" class="headerlink" title="调试与运行"></a>调试与运行</h4></li>
<li>第一次进行编译时Nuget会自动下载大约1G大小的依赖，请耐心等待，如果编译过程中出现“<strong>错误    8559    error C2220: 警告被视为错误…</strong>”，请参考<a href="http://115.29.54.164/?/question/281#!answer_form" target="_blank" rel="external">fisherman</a>的解决方法。</li>
<li>首次编译成功后，鼠标右击classfication项目，之后单击”Set as StartUp Project”选项，设置程序的启动项目<br><img src="http://img.blog.csdn.net/20160523200739474" alt="修改启动项目"></li>
<li>接着进入classfication项目的classfication.cpp文件中，拉到最下面，找到main函数，修改代码，在代码中指定模型、图片等文件的路径。然后设置断点，就可以可使用VS强大的调试功能一步步观察Caffe的运行过程了。</li>
</ul>
<p><img src="http://img.blog.csdn.net/20160523202031729" alt="修改main函数"></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Caffe/">Caffe</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-dl-practical-methodolog" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/04/23/dl-practical-methodolog/" class="article-date">
  	<time datetime="2016-04-22T17:44:36.000Z" itemprop="datePublished">2016-04-23</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/04/23/dl-practical-methodolog/">深度学习实用策略</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文翻译自DeepLearning balabala</p>
<h4 id="u5F15_u8A00"><a href="#u5F15_u8A00" class="headerlink" title="引言"></a>引言</h4><p>要想在实际生产中应用深度学习技术，仅仅知道算法的公式与原理是远远不够的。优秀的数据工作者需要能够根据特定的应用选择合适的算法，并能够根据实验过程中算法的反馈信息不断进行优化——是收集更多的数据，还是增加模型的复杂度，或者改变数据预处理方法，抑或是对程序进行调试等等。所有这些优化方法都需要耗费大量时间，没有方向性地乱撞显然是行不通的，需要有一些通用的、原则性的指导。</p>
<p>机器学习领域有各式各样的模型、训练算法与目标函数，这会给人一种“错觉”，对机器学习专家来说最重要的是掌握各式各样的机器学习技法和各个相关领域的数学知识。实际上，如果使用恰当，即使是烂大街的普通算法，也要比马马虎虎、一知半解使用的“高级”算法效果要好很多。而正确地使用算法很简单，掌握一些很简单的原则与策略就足够了。下面是推荐的机器学习系统设计流程：</p>
<ul>
<li>决定目标——首先要明确算法的性能衡量指标(原文是error metric)，以及要将这个衡量指标优化到什么程度。</li>
<li>尽快建立end to end的pipeline，包括恰当地估计性能指标。</li>
<li>找出系统的瓶颈，并有针对性优化。找出系统的哪个部分表现低于预期并分析原因——是因为过拟合还是因为欠拟合，还是数据或或代码出了问题。</li>
<li>不断地进行增量式调整——比如收集新的数据或者调整先验参数，甚至更换算法。</li>
</ul>
<p>为了详细介绍上述的流程，将使用经典的街景门牌号识别系统为例进行说明。街景车能拍下建筑的门牌号并记录拍摄时的位置，该系统识别图片中的门牌号，并在谷歌地图中的相应位置更新信息。相信随着读者一步步了解这个成功的商业应用系统是如何构建的，可以加深对上面提到的设计流程的理解。</p>
<h4 id="u6027_u80FD_u8861_u91CF_u6307_u6807"><a href="#u6027_u80FD_u8861_u91CF_u6307_u6807" class="headerlink" title="性能衡量指标"></a>性能衡量指标</h4><p>决定系统的性能衡量指标（即error metric）是相当重要的第一步，因为后续所有的工作都是围绕着这个指标进行的。另外，也要对系统的性能要达到何种程度做到心中有数。</p>
<p>系统的性能衡量指标跟算法要优化的损失函数多数情况下并不相同，比如简单二分类问题，损失函数可以是LR中交叉熵损失、后者SVM中使用hinge loss，或者是简单的感知机使用的0-1损失，但是我们对算法的性能衡量指标却可能是查全率、查准率或者是综合考虑查全率与查准率的…..。</p>
<p>需要明确的是，绝大多数系统都是做不到没有任何错误的。即使你有无限的多的训练数据而且能恢复真实的概率分布，也突破不了贝叶斯错误（Bayes error）的下界。因为特征含有的信息可能相对于输出变量来说并不完全(比如只知道)，或者系统本身就含有随机性。何况获取无限的训练数据也是不可能的。</p>
<p>训练数据的多少受到多种因素的制约。实际的应用存在训练数据量与性能的trade-off，收集数据是需要付出代价的，时间、金钱、人力都需要考虑，需要在数据量增加带来的受益与收集数据付出的成本之间进行权衡。如果目的是偏学术性的，要衡量不同算法的性能，那么使用的训练与测试数据是公认的benchmark，是不能够对数据随便进行更改的。</p>
<p>如何决定要将衡量指标优化到什么程度呢？如果是学术研究的话，至少要比当前的其他算法性能要好后者差不多吧，否则也没有意义。实际商业应用的话，考虑的因素就多一些，</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-C-中的虚继承" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/04/16/C-中的虚继承/" class="article-date">
  	<time datetime="2016-04-16T12:32:56.000Z" itemprop="datePublished">2016-04-16</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/04/16/C-中的虚继承/">C++中的虚</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>C++中主要有虚函数，纯虚函数和虚继承三种“虚”。之前的<a href="http://withwsf.github.io/2016/03/21/C-%E4%B8%AD%E7%9A%84%E8%99%9A%E5%87%BD%E6%95%B0%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/">一篇文章</a>中我们讲过虚函数的实现原理。下面继续讲一讲纯虚函数和虚继承。</p>
<h4 id="u7EAF_u865A_u51FD_u6570"><a href="#u7EAF_u865A_u51FD_u6570" class="headerlink" title="纯虚函数"></a>纯虚函数</h4><p>而对于纯虚函数与虚函数的区别，可以概括为————虚函数可以让派生类选择单纯继承接口还是同时继承接口与实现，而纯虚函数强制规定派生类只继承接口并自己实现函数。</p>
<ul>
<li><p>定义了纯虚函数的类称为抽象类，抽象类不能够被实例化</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> abstractClass&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">pure_virtual</span><span class="params">()</span></span>=<span class="number">0</span>; <span class="comment">//通用的纯虚函数声明方式</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
<li><p>纯虚函数一般只给出声明而不给出定义，如果要给出定义的话，形式如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> abstractClass::pure_virtual()&#123;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span>&lt;&lt;<span class="string">"This is abstractClass::pure_virtual !"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>只有给出纯虚函数实现的派生类才能够被实例化，否则派生类仍然是抽象类</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> child: <span class="keyword">public</span> abstractClass&#123;</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">pure_virtual</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">void</span> child::pure_virtual()&#123;</span><br><span class="line">  abstractClass::pure_virtual();<span class="comment">//这里只是距离如果纯虚函数给出了实现的话如何使用</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="u865A_u7EE7_u627F"><a href="#u865A_u7EE7_u627F" class="headerlink" title="虚继承"></a>虚继承</h4><ul>
<li>在普通继承中，派生类对象来将从基类对象继承来的非静态成员变量与自身的非静态成员变量放在同一块连续的内存中<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> Parent&#123;<span class="comment">//打印出基类对象中非静态成员变量与派生类对象中非静态成员变量在内存中的offset</span></span><br><span class="line">  <span class="comment">//可以帮助我们进行理解</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">int</span> a;</span><br><span class="line">    <span class="keyword">int</span> b;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">foo</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"parent"</span> &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> Child : <span class="keyword">public</span> Parent&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">int</span> c;</span><br><span class="line">    <span class="keyword">int</span> d;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">foo</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"child"</span> &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    Parent p;</span><br><span class="line">    Child c;</span><br><span class="line"></span><br><span class="line">    p.foo();</span><br><span class="line">    c.foo();</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Parent Offset a = "</span> &lt;&lt; (<span class="keyword">size_t</span>)&amp;p.a - (<span class="keyword">size_t</span>)&amp;p &lt;&lt; endl;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Parent Offset b = "</span> &lt;&lt; (<span class="keyword">size_t</span>)&amp;p.b - (<span class="keyword">size_t</span>)&amp;p &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Child Offset a = "</span> &lt;&lt; (<span class="keyword">size_t</span>)&amp;c.a - (<span class="keyword">size_t</span>)&amp;c &lt;&lt; endl;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Child Offset b = "</span> &lt;&lt; (<span class="keyword">size_t</span>)&amp;c.b - (<span class="keyword">size_t</span>)&amp;c &lt;&lt; endl;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Child Offset c = "</span> &lt;&lt; (<span class="keyword">size_t</span>)&amp;c.c - (<span class="keyword">size_t</span>)&amp;c &lt;&lt; endl;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Child Offset d = "</span> &lt;&lt; (<span class="keyword">size_t</span>)&amp;c.d - (<span class="keyword">size_t</span>)&amp;c &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    system(<span class="string">"pause"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>最终输出结果是：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">parent</span><br><span class="line">child</span><br><span class="line">Parent Offset <span class="tag">a</span> = <span class="number">8</span></span><br><span class="line">Parent Offset <span class="tag">b</span> = <span class="number">12</span></span><br><span class="line">Child Offset <span class="tag">a</span> = <span class="number">8</span></span><br><span class="line">Child Offset <span class="tag">b</span> = <span class="number">12</span></span><br><span class="line">Child Offset c = <span class="number">16</span></span><br><span class="line">Child Offset d = <span class="number">20</span></span><br></pre></td></tr></table></figure></p>
<ul>
<li>在多重继承中，如果继续使用单继承中的内存布局方法，可以会引起多种问题，需要另外进行设计。</li>
</ul>
<p>考虑如下的继承关系:<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> B&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;<span class="string">"B"</span>&lt;&lt;endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> C:<span class="keyword">public</span> B&#123;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> D:<span class="keyword">public</span> B&#123;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> E:<span class="keyword">public</span> C,<span class="keyword">public</span> D&#123;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">  E e;</span><br><span class="line">  e.foo();<span class="comment">//将会报错，比如在qt creator中编译器将抛出</span></span><br><span class="line">          <span class="comment">//error: request for member 'foo' is ambiguous e.foo()</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上面的代码中，派生类E有两个基类C和D，而C和D都有一个共同的基类B。这样e中就有两个B的对象，那么自然<br>在调用e.foo()的时候会产生歧义。另外，由于重复保存了B的对象，还会造成内存浪费。</p>
<p>为了防止上面的现象，引入了虚继承————继承基类时使用virtual关键字：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> B&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;<span class="string">"B"</span>&lt;&lt;endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> C:<span class="keyword">virtual</span> <span class="keyword">public</span> B&#123;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> D:<span class="keyword">virtual</span> <span class="keyword">public</span> B&#123;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> E:<span class="keyword">public</span> C,<span class="keyword">public</span> D&#123;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>“虚”可以认为意味着“在运行时决定”，使用虚继承的派生类在运行时才决定绑定哪个基类。C++并没有具体规定虚继承的<br>对象的内存布局，而是由编译器决定如何实现。一般情况下，会在虚继承的派生类对象中增添一个虚基类指针，指向基类对象。比如上面代码中E继承C和D时，会继承C和D的虚基类指针，而C和D的虚基类指针会指向同一个基类B。这样，就实现了E对象中只有一个B对象。</p>
<h4 id="u53C2_u8003_u8D44_u6599_uFF1A"><a href="#u53C2_u8003_u8D44_u6599_uFF1A" class="headerlink" title="参考资料："></a>参考资料：</h4><p><a href="https://wikipedia.kfd.me/zh-cn/%E8%99%9A%E5%87%BD%E6%95%B0_(%E7%A8%8B%E5%BA%8F%E8%AF%AD%E8%A8%80" target="_blank" rel="external">wikipedia-虚函数</a><br><a href="http://stackoverflow.com/questions/8672218/memory-layout-of-inherited-class" target="_blank" rel="external">memory layout of inherited class</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/C/">C++</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Caffe-with-Python-Layer" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/04/14/Caffe-with-Python-Layer/" class="article-date">
  	<time datetime="2016-04-14T03:10:01.000Z" itemprop="datePublished">2016-04-14</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/04/14/Caffe-with-Python-Layer/">在Caffe中使用Python Layer</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Caffe_u901A_u8FC7Boost_u4E2D_u7684Boost-Python_u6A21_u5757_u6765_u652F_u6301_u4F7F_u7528Python_u5B9A_u4E49Layer_uFF1A"><a href="#Caffe_u901A_u8FC7Boost_u4E2D_u7684Boost-Python_u6A21_u5757_u6765_u652F_u6301_u4F7F_u7528Python_u5B9A_u4E49Layer_uFF1A" class="headerlink" title="Caffe通过Boost中的Boost.Python模块来支持使用Python定义Layer："></a>Caffe通过Boost中的Boost.Python模块来支持使用Python定义Layer：</h4><ul>
<li>使用C++增加新的Layer<strong>繁琐</strong>、<strong>耗时</strong>而且很容易<strong>出错</strong></li>
<li><strong>开发速度</strong>与<strong>执行速度</strong>之间的<strong>trade-off</strong></li>
</ul>
<h4 id="u7F16_u8BD1_u652F_u6301Python_Layer_u7684Caffe"><a href="#u7F16_u8BD1_u652F_u6301Python_Layer_u7684Caffe" class="headerlink" title="编译支持Python Layer的Caffe"></a>编译支持Python Layer的Caffe</h4><p>如果是首次编译，修改Caffe根目录下的Makefile.cinfig，uncomment<br><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="label">WITH_PYTHON_LAYER:</span>=<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p> 如果已经编译过<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">make</span> clean</span><br><span class="line">WITH_PYTHON_LAYER=<span class="number">1</span> <span class="built_in">make</span>&amp;&amp; <span class="built_in">make</span> pycaffe</span><br></pre></td></tr></table></figure></p>
<h4 id="u4F7F_u7528Python_Layer"><a href="#u4F7F_u7528Python_Layer" class="headerlink" title="使用Python Layer"></a>使用Python Layer</h4><p>在网络的prototxt文件中添加一个Python定义的loss层如下：<br><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">layer&#123;</span><br><span class="line"><span class="keyword">type</span>: ’<span class="type">Python'</span></span><br><span class="line">name: <span class="symbol">'loss'</span></span><br><span class="line">top: <span class="symbol">'loss'</span></span><br><span class="line">bottom： ‘ipx’</span><br><span class="line">bottom: <span class="symbol">'ipy'</span></span><br><span class="line">python_param&#123;</span><br><span class="line">#<span class="keyword">module</span>的名字，通常是定义<span class="type">Layer</span>的.py文件的文件名，需要在$<span class="type">PYTHONPATH</span>下</span><br><span class="line"><span class="keyword">module</span>: <span class="symbol">'pyloss'</span></span><br><span class="line">#layer的名字---<span class="keyword">module</span>中的类名</span><br><span class="line">layer: <span class="symbol">'EuclideanLossLayer'</span></span><br><span class="line">&#125;</span><br><span class="line">loss_weight: <span class="number">1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="u5B9A_u4E49Python_Layer"><a href="#u5B9A_u4E49Python_Layer" class="headerlink" title="定义Python Layer"></a>定义Python Layer</h4><p>根据上面的要求，我们在$PYTHONPAT在创建pyloss.py，并在其中定义EuclideanLossLayer。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> caffe</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EuclideadLossLayer</span><span class="params">(caffe.Layer)</span>:</span><span class="comment">#EuclideadLossLayer没有权值，反向传播过程中不需要进行权值的更新。如果需要定义需要更新自身权值的层，最好还是使用C++</span></span><br><span class="line">       <span class="function"><span class="keyword">def</span> <span class="title">setup</span><span class="params">(self,bottom,top)</span>:</span></span><br><span class="line">           <span class="comment">#在网络运行之前根据相关参数参数进行layer的初始化</span></span><br><span class="line">           <span class="keyword">if</span> len(bottom) !=<span class="number">2</span>:</span><br><span class="line">              <span class="keyword">raise</span> exception(<span class="string">"Need two inputs to compute distance"</span>)</span><br><span class="line">       <span class="function"><span class="keyword">def</span> <span class="title">reshape</span><span class="params">(self,bottom,top)</span>:</span></span><br><span class="line">           <span class="comment">#在forward之前调用，根据bottom blob的尺寸调整中间变量和top blob的尺寸</span></span><br><span class="line">           <span class="keyword">if</span> bottom[<span class="number">0</span>].count !=bottom[<span class="number">1</span>].count:</span><br><span class="line">              <span class="keyword">raise</span> exception(<span class="string">"Inputs must have the same dimension."</span>)</span><br><span class="line">           self.diff=np.zeros_like(bottom[<span class="number">0</span>].date,dtype=np.float32)</span><br><span class="line">           top[<span class="number">0</span>].reshape(<span class="number">1</span>)</span><br><span class="line">       <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,bottom,top)</span>:</span></span><br><span class="line">           <span class="comment">#网络的前向传播</span></span><br><span class="line">            self.diff[...]=bottom[<span class="number">0</span>].data-bottom[<span class="number">1</span>].data</span><br><span class="line">            top[<span class="number">0</span>].data[...]=np.sum(self.diff**<span class="number">2</span>)/bottom[<span class="number">0</span>].num/<span class="number">2.</span></span><br><span class="line">       <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self,top,propagate_down,bootm)</span>:</span></span><br><span class="line">            <span class="comment">#网络的前向传播</span></span><br><span class="line">             <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">                 <span class="keyword">if</span> <span class="keyword">not</span> propagate_down[i]:</span><br><span class="line">                         <span class="keyword">continue</span></span><br><span class="line">                 <span class="keyword">if</span> i==<span class="number">0</span>:</span><br><span class="line">                    sign=<span class="number">1</span></span><br><span class="line">                 <span class="keyword">else</span>:</span><br><span class="line">                    sign=-<span class="number">1</span></span><br><span class="line">                  bottom[i].diff[...]=sign*self.diff/bottom[i].num</span><br></pre></td></tr></table></figure></p>
<h4 id="u539F_u7406_u6D45_u6790"><a href="#u539F_u7406_u6D45_u6790" class="headerlink" title="原理浅析"></a>原理浅析</h4><p>阅读caffe源码python<em>layer.hpp可以知道，类PythonLayer继承自Layer，并且新增私有变量boost::python::object self</em>来表示我们自己定义的python layer的内存对象。</p>
<p>类PythonLayer类的成员函数LayerSetUP, Reshape, Forward_cpu和Backward_cpu分别是对我们自己定义的python layer中成员函数setup, reshape, forward和backward的封装调用。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> PythonLayer : <span class="keyword">public</span> Layer&lt;Dtype&gt; &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  PythonLayer(PyObject* self, <span class="keyword">const</span> LayerParameter&amp; param)</span><br><span class="line">      : Layer&lt;Dtype&gt;(param), self_(bp::handle&lt;&gt;(bp::borrowed(self))) &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">LayerSetUp</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span><br><span class="line">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Disallow PythonLayer in MultiGPU training stage, due to GIL issues</span></span><br><span class="line">    <span class="comment">// Details: https://github.com/BVLC/caffe/issues/2936</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;phase_ == TRAIN &amp;&amp; Caffe::solver_count() &gt; <span class="number">1</span></span><br><span class="line">        &amp;&amp; !ShareInParallel()) &#123;</span><br><span class="line">      LOG(FATAL) &lt;&lt; <span class="string">"PythonLayer is not implemented in Multi-GPU training"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    self_.attr(<span class="string">"param_str"</span>) = bp::str(</span><br><span class="line">        <span class="keyword">this</span>-&gt;layer_param_.python_param().param_str());</span><br><span class="line">    self_.attr(<span class="string">"setup"</span>)(bottom, top);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Reshape</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span><br><span class="line">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span> </span>&#123;</span><br><span class="line">    self_.attr(<span class="string">"reshape"</span>)(bottom, top);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">bool</span> <span class="title">ShareInParallel</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>-&gt;layer_param_.python_param().share_in_parallel();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">const</span> <span class="keyword">char</span>* <span class="title">type</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="string">"Python"</span>; &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">protected</span>:</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Forward_cpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span><br><span class="line">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span> </span>&#123;</span><br><span class="line">    self_.attr(<span class="string">"forward"</span>)(bottom, top);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Backward_cpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span><br><span class="line">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)</span> </span>&#123;</span><br><span class="line">    self_.attr(<span class="string">"backward"</span>)(top, propagate_down, bottom);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  bp::object self_;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p><a href="http://chrischoy.github.io/research/making-caffe-layer/" target="_blank" rel="external">进一步了解使用C++创建新layer</a><br><strong>参考资料</strong></p>
<ul>
<li><a href="https://github.com/BVLC/caffe/pull/1703" target="_blank" rel="external">https://github.com/BVLC/caffe/pull/1703</a></li>
<li><a href="https://gist.github.com//shelhamer/8d9a94cf75e6fb2df221" target="_blank" rel="external">https://gist.github.com//shelhamer/8d9a94cf75e6fb2df221</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Caffe/">Caffe</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-C-中的虚函数实现原理" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/03/21/C-中的虚函数实现原理/" class="article-date">
  	<time datetime="2016-03-21T12:21:47.000Z" itemprop="datePublished">2016-03-21</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/03/21/C-中的虚函数实现原理/">C++中的虚函数实现原理</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>参考资料：<a href="http://www.hanese.nl/~ewout/ESE/INF2/CPP_onder_de_motorkap.pdf" target="_blank" rel="external">C++ Under the hood</a></p>
</blockquote>
<h3 id="u865A_u51FD_u6570_u662FC++_u4E2D_u591A_u6001_u7279_u6027_u7684_u5B9E_u73B0_u57FA_u7840_u3002"><a href="#u865A_u51FD_u6570_u662FC++_u4E2D_u591A_u6001_u7279_u6027_u7684_u5B9E_u73B0_u57FA_u7840_u3002" class="headerlink" title="虚函数是C++中多态特性的实现基础。"></a>虚函数是C++中<a href="http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001386820044406b227b3e751cc4d5190420d17a2dc6353000" target="_blank" rel="external">多态特性</a>的实现基础。</h3><h3 id="0-_u9996_u5148_u5B9A_u4E49_u4E00_u4E2A_u57FA_u7C7BB_u4E0E_u6D3E_u751F_u7C7BD_u6765_u8F85_u52A9_u8BF4_u660E_u95EE_u9898_uFF1A"><a href="#0-_u9996_u5148_u5B9A_u4E49_u4E00_u4E2A_u57FA_u7C7BB_u4E0E_u6D3E_u751F_u7C7BD_u6765_u8F85_u52A9_u8BF4_u660E_u95EE_u9898_uFF1A" class="headerlink" title="0.首先定义一个基类B与派生类D来辅助说明问题："></a>0.首先定义一个基类B与派生类D来辅助说明问题：</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">   <span class="keyword">class</span> B&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">B();</span><br><span class="line"><span class="keyword">virtual</span> ~ B();</span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">fun1</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">fun2</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">fun3</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> fun4 <span class="title">const</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> D:<span class="keyword">public</span> B&#123;</span><br><span class="line">D();</span><br><span class="line">~D();</span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">fun1</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">fun5</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="1-_u865A_u51FD_u6570_u7684_u539F_u7406_u53EF_u4EE5_u6982_u62EC_u5982_u4E0B_uFF1A"><a href="#1-_u865A_u51FD_u6570_u7684_u539F_u7406_u53EF_u4EE5_u6982_u62EC_u5982_u4E0B_uFF1A" class="headerlink" title="1.虚函数的原理可以概括如下："></a>1.虚函数的原理可以概括如下：</h4><ul>
<li>对于含有虚函数的类，编译器会为其建立一个虚函数表(vtbl)，表中的元素是指向虚函数代码所在位置的指针。<br><img src="http://7xs45i.com1.z0.glb.clouddn.com/withwsfScreenshot%20from%202016-03-21%2020-41-48.png" alt="B的虚函数表"></li>
<li>派生类的虚函数表继承自基类并添加自己的虚函数（示例代码中D新定义的虚函数D::fun5将会被添加到虚函数表）。如果基类的虚函数被派生类重载（在上面的示例代码中基类的虚函数fun1被派生类的虚函数fun1覆盖），那么由派生类虚函数代替基类虚函数的位置（B::~B和B::fun1将被D::~D和D::fun1代替）。<br><img src="http://7xs45i.com1.z0.glb.clouddn.com/withwsfSelection_001.png" alt="D的虚函数表"></li>
<li>含有虚函数的类的每个对象中都有一个指针（称为vptr,一般放在对象所在内存的首地址），指向类的虚函数表。<br><img src="http://7xs45i.com1.z0.glb.clouddn.com/withwsfSelection_002.png" alt="vptr与指向的vtbl"></li>
<li>由于派生对其继承自基类的虚函数表进行了改写，所以当基类的指针或引用被绑定到派生类对象时，尽管静态类型是基类，却可以调用派生类覆盖后的虚函数。</li>
</ul>
<h3 id="2-_u591A_u91CD_u7EE7_u627F_u65F6_u7684_u60C5_u51B5"><a href="#2-_u591A_u91CD_u7EE7_u627F_u65F6_u7684_u60C5_u51B5" class="headerlink" title="2.多重继承时的情况"></a>2.多重继承时的情况</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> B1&#123;</span><br><span class="line">B1()</span><br><span class="line"><span class="keyword">virtual</span> ~B();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">class</span> B2&#123;</span><br><span class="line">B2()</span><br><span class="line"><span class="keyword">virtual</span> ~B2();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> D:<span class="keyword">public</span> B1, <span class="keyword">public</span> B2&#123;</span><br><span class="line">~D();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>编译器会为继承自多个基类的派生类产生与基类个数相同的虚函数表，并在继承自第一个基类的虚函数表上添加派生类新定义的虚函数。</li>
<li>派生类的对象中也会含有与基类个数相同的vptr。</li>
<li>将派生类对象绑定到不同的基类指针（或引用）时，编译器将根据基类指针（或引用）的不同选择使用不同的虚函数表（派生类的指针(或引用)与第一个基类的指针(或引用)使用同一个虚函数表）。！<br><img src="http://7xs45i.com1.z0.glb.clouddn.com/withwsfSelection_003.png" alt="多重继承下的虚函数表"></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/C/">C++</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Python在Windows环境下处理文件路径问题最佳实践" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/12/30/Python在Windows环境下处理文件路径问题最佳实践/" class="article-date">
  	<time datetime="2015-12-30T06:45:38.000Z" itemprop="datePublished">2015-12-30</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/12/30/Python在Windows环境下处理文件路径问题最佳实践/">Python在Windows环境下处理文件路径问题最佳实践</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Windows中路径分隔符是反斜线’\’，而在Python中’\’又有转义符的作用，因而直接从windows资源管理器复制的路径在Python中是不能正常识别的。</p>
<ul>
<li><strong>最优实践：</strong><br>使用os.path.join来join不同的路径，比如<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">path=os.path.join(dirpath,filepath)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>也可以使用os.sep，python会根据不同的系统自动选择合适的路径分隔符<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">path=dirpath+os.sep+filepath</span><br></pre></td></tr></table></figure></p>
<ul>
<li><strong>次优方案</strong><br>可以将所有路径都使用正斜线’/‘，不管在Windows和Linux中都适用.</li>
<li><strong>不要使用</strong><br>在引用的字符串前面加上’r’可以将转义字符串(escaped strings )转换为原始字符串(raw strings)，可以解决部分问题。比如：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">file=open(<span class="string">'c:\myfile'</span>) <span class="comment">#打开错误</span></span><br><span class="line">file=open(<span class="string">r'c:\myfile'</span>)<span class="comment">#可以正确打开</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>但是’r’是为了方便书写正则表达式而不是为了解决Windows下文件路径问题而设计的特性，所以会遇到一下问题</p>
<blockquote>
<p>file=open(r’c:\dir\’+’myfile’)</p>
</blockquote>
<p>上述代码是错误的，虽然’\’失去了转义作用，但仍然有保护作用，其后的‘’’并不会被视为closing delimiter。</p>
<p>由此可见，使用r不仅没有完全解决问题，还会引入新的问题，所以最好不要使用。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python/">Python</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-数据挖掘中的数据预处理" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/12/29/数据挖掘中的数据预处理/" class="article-date">
  	<time datetime="2015-12-29T14:13:02.000Z" itemprop="datePublished">2015-12-29</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/12/29/数据挖掘中的数据预处理/">数据挖掘中的数据预处理</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="0-0__u5173_u4E8E__u300ADATA_MINING_u2014Concepts_and_Techniques_u300B"><a href="#0-0__u5173_u4E8E__u300ADATA_MINING_u2014Concepts_and_Techniques_u300B" class="headerlink" title="0.0 关于 《DATA MINING—Concepts and Techniques》"></a>0.0 关于 《DATA MINING—Concepts and Techniques》</h3><p>《DATA MINING—Concepts and Techniques》是经典的数据挖掘入门书籍，内容囊括数据挖掘的基本概念、数据的预处理、数据的存储、数据中模式的挖掘、分类、聚类、异常检测等方面，作者是著名的韩家炜教授。数据的预处理在真实世界数据中是非常关键的一步，它既是不同数据挖掘应用的共同起点，又很大程度上影响了数据挖掘应用的效果。我将翻译、整理这本书中关于数据预处理的部分，如果有纰漏欢迎指正。</p>
<h3 id="0-1__u6570_u636E_u9884_u5904_u7406_u7EFC_u8FF0"><a href="#0-1__u6570_u636E_u9884_u5904_u7406_u7EFC_u8FF0" class="headerlink" title="0.1 数据预处理综述"></a>0.1 数据预处理综述</h3><ul>
<li>由于真实世界中的数据来源复杂、体积巨大，往往难以避免地存在缺失、噪声、不一致（inconsistencies）等问题。</li>
<li>当数据的维数过高时还会存在所谓的“<a href="http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf" target="_blank" rel="external">维数诅咒（Curse of dimensionality）</a>”问题，过高的维度不仅增加了计算量，反而可能会降低算法的效果。</li>
<li>有些算法对数据存在特殊的要求，比如KNN、Neural Networks、Clustering等基于距离（distance based）的算法在数据进行normalize之后效果会提升。</li>
</ul>
<p>解决上述问题需要在将数据送入算法之前进行预处理，具体包括<strong>Data Cleaning</strong>,<strong>Data Intergation</strong>，<strong>Data reduction</strong>,<strong>Data Transformation and Data Discretization</strong>等步骤。下面将对各个部分详细展开。</p>
<h3 id="1-__u6570_u636E_u6E05_u6D17_uFF08Data_Cleaning_uFF09"><a href="#1-__u6570_u636E_u6E05_u6D17_uFF08Data_Cleaning_uFF09" class="headerlink" title="1. 数据清洗（Data Cleaning）"></a>1. 数据清洗（Data Cleaning）</h3><p>数据清洗的主要作用是处理数据的某些纪录值缺失，平滑数据中的噪声、发现异常值，改正不一致。</p>
<h4 id="u503C_u7F3A_u5931"><a href="#u503C_u7F3A_u5931" class="headerlink" title="值缺失"></a>值缺失</h4><p>针对数据中某些记录的值缺失问题（比如用户销售数据中，有些顾客的收入信息缺失，有些顾客的年龄信息缺失），可以采用如下的方式：</p>
<ul>
<li>忽略存在缺失值的记录。在分类问题中，如果样本（本文中，’一个样本’和’一条记录’是同义词）的label缺失了，那么这个样本一定是要丢弃的。其他情况下，除非一条记录缺失了多个值，否则这种简单粗暴的方法往往效果不好，尤其当不同的属性值缺失状况相差很大时，效果会更差。</li>
<li>手动填写缺失的值。根据经验手工补上缺失的值，但是这种活儿谁愿意干呢？</li>
<li>用全局的常量来替代缺失值。再拿用户销售数据做例子，对于年龄缺失的纪录年龄全用unknow来代替，但是对于数据挖掘算法来说，young和unknow并没有什么本质的不同，都只是属性值的一种而已，所以所有年龄缺失的纪录在算法看来都是同一种年龄—-“unknow”,这反而可能会降低算法的效果。</li>
<li>使用中值和均值来替代缺失值。某种属性的中值和均值代表了此属性的平均趋势，用它们来代替缺失值不失为一种可行的方案。但是，当属性是“红、绿、蓝”这种离散值时显然不存在中值、均值的概念。</li>
<li>使用class-specific的中值和均值来代替缺失值。在有监督问题中，一个基本假设是label相同的样本属性也相似，那么，某个样本的缺失值，用其所在类别内的所有样本的在此属性的均值或中值来代替理应效果更好。</li>
<li>使用最可能的值来填充缺失值。用此样本的其他属性来推断缺失属性的最可能值，实际上这就变成了一个回归或分类问题（属性值连续时是回归问题，离散时是分类问题）。实际中常用<a href="https://en.wikipedia.org/wiki/Bayesian_inference" target="_blank" rel="external">贝叶斯推理</a>或决策树来解决上述问题。</li>
</ul>
<p>上述的第3-第6种方法都会引入偏差，因为补充的缺失值跟真实值很可能不同。第六种方法在现实中非常流行，因为它在推断缺失值时使用的信息最多，那么结果理应更准确。不过需要注意的是，有时缺失值也会提供有用的信息，比如在信用卡申请用户数据中，没有驾照号码很可能是因为没有汽车，而是否有汽车是评价信用等级很有用的信息。</p>
<h4 id="u566A_u58F0_uFF08noise_uFF09"><a href="#u566A_u58F0_uFF08noise_uFF09" class="headerlink" title="噪声（noise）"></a>噪声（noise）</h4><p>噪声是混在观测值的错误(error)或误差(variance)，具体去噪方式有以下几种：</p>
<ul>
<li>Binning。Data Bininig，又称为Bucketing，从字面意思来展开，就是把样本点按照一定的准则分配到不同的bin(bucket)中去，然后对每个样本点根据其所在bin内样本点的分布来赋一个新值，同一个bin的样本点被赋予的新值是一致的。对于一维数据，bin可以按照区间大小划分，也可以按照data frequency来划分，而每个bin的值可以选择分布在其中样本的均值、中值或者边界值。另外，CNN中的max-pooling层，也属于data binning的范畴，典型的max-pooling层bin的尺寸为2*2，选择每个bin中的最大值作为bin四个值的新值。</li>
<li>回归。如果变量之间存在依赖关系，即y=f(x)，那么我们可以设法求出依赖关系f，从而根据x来预测y，这也是回归问题的实质。实际中更常见的假设是P(y)=N(f(x))，N是正态分布。假设y是观测值且存在噪声，如果我们能求出x和y之间的依赖关系，从而根据x来更新y的值，就可以去除其中的随机噪声，这就是回归去噪的原理。</li>
<li>异常值检测。数据中的噪声可能有两种，一种是随机误差，另外一种可能是错误，比如我们手上有一份顾客的身高数据，其中某一位顾客的身高纪录是20m，很明显，这是一个错误，如果这个一场的样本进入了我们训练数据可能会对结果产生很大影响，这也是去噪中使用异常值检测的意义所在。当然，异常值检测远不止去噪这么一个应用，网络入侵检测、视频中行人异常行为检测、欺诈检测等都是异常值检测的应用。异常值检测方法也分为有监督，无监督和半监督方法，这里不再详细展开。<h3 id="2-__u6570_u636E_u878D_u5408"><a href="#2-__u6570_u636E_u878D_u5408" class="headerlink" title="2. 数据融合"></a>2. 数据融合</h3>所谓数据融合就是将不同来源的、异质的数据融合到一起。良好的数据融合可以减少数据中的冗余(redundacies)和不一致性(inconsistence)，进而提升后续步骤的精度和速度。数据融合包括如下几个步骤：<h4 id="u5B9E_u4F53_u8BC6_u522B_u95EE_u9898_uFF08Entity_Identification_Problem_uFF09"><a href="#u5B9E_u4F53_u8BC6_u522B_u95EE_u9898_uFF08Entity_Identification_Problem_uFF09" class="headerlink" title="实体识别问题（Entity  Identification Problem）"></a>实体识别问题（Entity  Identification Problem）</h4>实体识别中最主要的问题匹配不同的数据源中指向现实世界相同实体的纪录。比如分析有不同销售员纪录的14年和15年两年的销售数据，由于不同的销售员有不同的纪录习惯，顾客的名字纪录方式并不一样，一个销售员喜欢纪录全名（例如 Wardell Stephen Curry II），另外一个销售员喜欢将中间名省略（Wardell S Curry II ），虽然Wardell Stephen Curry II和Wardell S Curry II是现实世界中是同一名顾客，但计算机会识别为两位不同的顾客，解决这个问题就需要Entity Identification。一个常用的Entity Indentification Problem的解决算法是<a href="http://www.mit.edu/~andoni/LSH/" target="_blank" rel="external">LSH算法</a><br>另外一个问题是Schema integration, Schenma在这里指使用DBMS支持的形式化语言对一个数据库的结构化描述，Schema是构建一个数据库的蓝图。Schema intergration则是指，将若干个Schema合成一个global Schema，这个global Schema可以表达所有子Schema的要求（也就是一个总的蓝图）。属性的metadata（比如名称、取值范围、空值处理方法）可以帮助减少Schema Intergration的错误。<h4 id="u5197_u4F59_u548C_u76F8_u5173_u6027_u5206_u6790"><a href="#u5197_u4F59_u548C_u76F8_u5173_u6027_u5206_u6790" class="headerlink" title="冗余和相关性分析"></a>冗余和相关性分析</h4>当能够从样本的一个或多个属性推导出另外的属性的时候，那么数据中就存在冗余。检测冗余的一种方法是相关性分析—-给定要进行检测的两个属性，相关性分析可以给出一个属性隐含(<strong>imply</strong>)另外一个属性的程度。对于标称型（<strong>Nominal</strong>）数据，可以使用$\chi^2$检验，而对于数值数据，可以根据方差和相关系数来分析。</li>
<li>标称数据的$\chi^2$相关性检验。<br>假设有A和B两个属性，A有$a_1,a_2,…a_c$共c种不同的取值，B有$b_1,b_2,…b_r$共r种不同的取值。我们可以为属性A和B建立一个列联表(<strong>contingency table</strong>)C，所谓列联表，就是一个r*c的矩阵，位置（i,j）代表属性A的值$a_i$和属性B的值$b_i$在样本中同时出现(事件(A=$a_i$,B=$b_j$)发生)的频率$o_ij$。属性A和属性B的$\chi^2$值可以通过下面的式子计算：<blockquote>
<p><span>$\chi^2=\sum_{i=1}^{c}\sum_{j=1}^{r}\frac{(o_{ij}-e_{ij})^2}{e_{ij}}$</span><!-- Has MathJax -->     (1)</p>
</blockquote>
</li>
</ul>
<p>其中$o_ij$是联合事件(A=$a_i$,B=$b_j$)发生的频率，$e_ij$是期望频率，用如下的公式计算：</p>
<blockquote>
<p><span>$e_{ij}=\frac{count(A=a_i)\times count(B=b_j)}{n}$</span><!-- Has MathJax -->                          (2)</p>
</blockquote>
<ul>
<li>数值数据的相关系数<br>假设有n个样本，$S_1,S_2,…S_n$,样本$S_i$的A，B两种属性的值分别是$a_i,b_i$，那么属性A和B的相关系数定义是：<blockquote>
<p><span>$r_{A,B}=\frac{\sum_{i=1}^{n}(a_i-\overline{A})(b_i-\overline{B})}{n\delta_A\delta_B}=\frac{\sum_{i=n}^{n}(a_i b_i)-n\overline{A} \overline{B}}{n\delta_A\delta_B}$</span><!-- Has MathJax -->                                                                                             （3）</p>
</blockquote>
</li>
</ul>
<p>当相关系数是正的时候表示属性A和属性B正相关，当相关系数是负的时候属性A和属性B负相关，注意，相关关系并不等同于因果关系。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/数据挖掘/">数据挖掘</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/阅读经典/">阅读经典</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-在torch中使用cuda进行训练" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/12/29/在torch中使用cuda进行训练/" class="article-date">
  	<time datetime="2015-12-29T07:50:33.000Z" itemprop="datePublished">2015-12-29</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/12/29/在torch中使用cuda进行训练/">在torch中使用cuda进行训练</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="0-__u8BF4_u660E"><a href="#0-__u8BF4_u660E" class="headerlink" title="0. 说明"></a>0. 说明</h3><p>本文介绍<strong>hdf5文件的读取</strong>、<strong>网络的定义</strong>与<strong>训练</strong>和<strong>测试</strong>，并强调了如何使用<strong>cuda</strong>对网络的训练、测试进行加速。</p>
<h3 id="1-__u8FC7_u7A0B_u4E0E_u4EE3_u7801"><a href="#1-__u8FC7_u7A0B_u4E0E_u4EE3_u7801" class="headerlink" title="1. 过程与代码"></a>1. 过程与代码</h3><ul>
<li><strong>1 安装hdf5</strong><br>按照<a href="https://github.com/deepmind/torch-hdf5/blob/master/doc/usage.md" target="_blank" rel="external">官方tutorial</a>安装torch对hdf5格式文件的支持。</li>
<li><p><strong>2 引入必要 package 并读入hdf5数据</strong></p>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">require</span> <span class="string">'torch'</span>;<span class="comment">----如果是在itorch或itorch notbook中自动引入torch</span></span><br><span class="line"><span class="built_in">require</span> <span class="string">'hdf5'</span>;<span class="comment">----hdf5支持</span></span><br><span class="line"><span class="built_in">require</span> <span class="string">'nn'</span>; <span class="comment">----neural network modules 的实现</span></span><br><span class="line"><span class="built_in">require</span> <span class="string">'cutorch'</span>;<span class="comment">----cuda backend支持</span></span><br><span class="line"><span class="built_in">require</span> <span class="string">'cunn'</span>;<span class="comment">----neural network modules的cuda实现</span></span><br></pre></td></tr></table></figure>
<p>下面读入数据，假设train和test数据都是按照我<a href="http://withwsf.github.io/2015/12/23/torch-hdf5/">上篇文章</a>的格式所存储：</p>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">myFiletrian=hdf5.open(<span class="string">'mytrain.h5'</span>,<span class="string">'r'</span>)<span class="comment">----读取hdf5文件</span></span><br><span class="line">myFiletest=hdf5.open(<span class="string">'mytest.h5'</span>,<span class="string">'r'</span>)</span><br><span class="line"><span class="comment">----将读入的hdf5存到两个Table： trainset和testset中</span></span><br><span class="line">trainset=&#123;data=myFiletrian:read(<span class="string">'data'</span>):all(),label=myFiletrian:read(<span class="string">'label'</span>):all():byte()&#125;</span><br><span class="line">testset=&#123;data=myFiletest:read(<span class="string">'data'</span>):all(),label=myFiletest:read(<span class="string">'label'</span>):all():byte()&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>3 重载trainset的_index操作符并添加成员函数size()</strong><br>Lua中也存在面向对象的概念，Lua中的类可以通过Table+function模拟出来。这里将trainset视作一个类的对象，torch要求这个类有方法trainset:size()可以返回样本个数，使用操作符trainset[i]时返回第i个样本。</p>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">setmetatable</span>(trainset, <span class="comment">----重载index操作符</span></span><br><span class="line">    &#123;__index = <span class="function"><span class="keyword">function</span><span class="params">(t, i)</span></span> </span><br><span class="line">                    <span class="keyword">return</span> &#123;t.data[i], t.label[i]&#125; </span><br><span class="line">                <span class="keyword">end</span>&#125;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">trainset:size</span><span class="params">()</span></span><span class="comment">----添加成员函数 size()</span></span><br><span class="line">    <span class="keyword">return</span> self.data:size(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>4 建立一个网络</strong><br>我使用Torch的主要原因是因为Torch有3D conv模块，所以这里以模仿<strong>LeNet</strong>的3D conv网络为例：</p>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">net=nn.Sequential()</span><br><span class="line">net:add(nn.VolumetricConvolution(<span class="number">3</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>))<span class="comment">----添加3D conv层，输入3个feature cube，输出5个feature cube，filter size为3*3*3</span></span><br><span class="line">net:add(nn.VolumetricMaxPooling(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>))<span class="comment">----添加3D MaxPooling层</span></span><br><span class="line">net:add(nn.VolumetricConvolution(<span class="number">5</span>,<span class="number">16</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>))</span><br><span class="line">net:add(nn.VolumetricMaxPooling(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">net:add(nn.View(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>*<span class="number">4</span>))<span class="comment">----连接全连接层之前要使用nn.View()进行reshpe，nn.View()里面的参数需要根据</span></span><br><span class="line"><span class="comment">---（接上）数据的尺寸和前面的层来计算，比如在这里我是用的训练数据是3*24*24*24的cube,分别经过3*3*3和4*4*4 filter尺寸的两次卷积和两次maxpooling之后最终的输出是16个4*4*4*的feature cube ，那么nn.View()里面的参数就应该是16*4*4*4</span></span><br><span class="line">net:add(nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>*<span class="number">4</span>,<span class="number">120</span>))</span><br><span class="line">net:add(nn.Linear(<span class="number">120</span>,<span class="number">84</span>))<span class="comment">----nn.Linear(a,b)，设置a个输入neurons和b个输出neurons的全全连接层</span></span><br><span class="line">net:add(nn.Linear(<span class="number">84</span>,<span class="number">7</span>))</span><br><span class="line">net:add(nn.LogSoftMax())<span class="comment">-----输出每个类别概率P的log函数值log(P)</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>5 建立一个Solver</strong></p>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">criterion=nn.ClassNLLCriterion()<span class="comment">----使用negatice log--likehood criterion ,即计算cross-entropy损失</span></span><br><span class="line"><span class="comment">----注意！Torch中训练数据的Label应该从1开始，比如有四类样本，那么label应该是1、2、3、4，不能从0开始，否则报错</span></span><br><span class="line">trainer=nn.StochasticGradient(net,criterion)<span class="comment">----使用随机梯度下降</span></span><br><span class="line">trainer.learningRate=<span class="number">0.001</span><span class="comment">----设置solver参数</span></span><br><span class="line">trainer.maxIteration=<span class="number">10</span><span class="comment">----迭代epoch数</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>6 将net、criterion、trainset和testset移动到GPU中</strong><br>如果要使用GPU加速，net、criterion、trainset和testset都需要移动到CPU内存中去</p>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">trainset.data=trainset.data:cuda()</span><br><span class="line">trainset.label=trainset.label:cuda()</span><br><span class="line">testset.label=trainset.label:cuda()</span><br><span class="line">testset.data=testset.data:cuda()</span><br><span class="line">criterion=criterion:cuda()</span><br><span class="line">net=net:cuda()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>以trainset为例，移动到GPU之后是形式如下的对象</p>
<blockquote>
<p>{<br>  data : CudaTensor - size: 10500x3x24x24x24<br>  size : function: 0x42832ef0<br>  label : CudaTensor - size: 10500x1<br>}</p>
</blockquote>
<ul>
<li><strong>7 训练并测试网络</strong><br>有了上面的准备，网络的训练十分简单：<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer:train(trainset)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<blockquote>
<p>训练过程中输出如下：<br>Out[18]:StochasticGradient: training<br>Out[18]:current error = 1.9459465204875<br>Out[18]:current error = 1.941716547001<br>Out[18]:current error = 1.921697193475<br>Out[18]:current error = 1.8641934820811<br>Out[18]:current error = 1.5424795499416<br>Out[18]:current error = 1.3549344599815<br>Out[18]:current error = 1.2374953328995<br>Out[18]:current error = 1.152671923592<br>Out[18]:current error = 1.094293069022<br>Out[18]:current error = 1.0469601832571<br>StochasticGradient: you have reached the maximum number of iterations<br>training error = 1.0469601832571</p>
</blockquote>
<p>由于设置的最大epoch是10，所以在对trainset迭代了10次之后训练就结束了，接下来是网络的测试：<br><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i=<span class="number">1</span>,<span class="number">3500</span> <span class="keyword">do</span> <span class="comment">----我的测试集有3500个cube</span></span><br><span class="line">    <span class="keyword">local</span> groundtruth = testset.label[i][<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">local</span> prediction = net:forward(testset.data[i])</span><br><span class="line">    <span class="keyword">local</span> confidences, indices = torch.sort(prediction, <span class="keyword">true</span>)</span><br><span class="line">    <span class="keyword">if</span> groundtruth == indices[<span class="number">1</span>] <span class="keyword">then</span></span><br><span class="line">       correct = correct + <span class="number">1</span> </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="built_in">print</span>(correct, <span class="number">100</span>*correct/<span class="number">3500</span> .. <span class="string">' % '</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="2-__u603B_u7ED3"><a href="#2-__u603B_u7ED3" class="headerlink" title="2. 总结"></a>2. 总结</h3><p>使用Torch训练过程包括数据载入，网络定义，trainer定义，训练、测试几个部分，如果使用GPU加速，要将数据等对象移动到GPU中。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Torch/">Torch</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2016 Withwsf
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: true
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js" type="text/javascript"></script>
<script src="/js/main.js" type="text/javascript"></script>






<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>